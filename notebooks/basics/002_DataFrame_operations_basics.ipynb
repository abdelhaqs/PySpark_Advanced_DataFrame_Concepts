{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame basic operations\n",
    "### Dr. Tirthajyoti Sarkar, Fremont, CA, July 2020 (updated)\n",
    "In this notebook, we go through basic operations that can be performed with a Spark DataFrame object. We will use a .CSV file of stock prices to illustrate the various useful methods that are associated with a Spark DataFrame.\n",
    "---\n",
    "https://github.com/tirthajyoti/Spark-with-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A `SparkSession` app, reading CSV, and the Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark1 = SparkSession.builder.appName('Ops').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Apple stock CSV file. Note we have an option of inferring the schema for CSV. We also have the option to set `header` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark1.read.csv('../datasets/appl_stock.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Show the columns\n",
    "\n",
    "The `columns` attribute returns a simple Python list of column names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Spark DataFrames have separate `Column` and `Row` types \n",
    "Instead of generic Pandas series, Spark DataFrames have separate `Column` and `Row` types. This is ensure thet can handle special data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df['High'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.head(2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### The `select` method to select particular columns\n",
    "\n",
    "We can use this method to actually select the DataFrame columns and see them. Note that we still have to use the `show` method to actually output the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[High: double]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('High')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              High|\n",
      "+------------------+\n",
      "|        214.499996|\n",
      "|        215.589994|\n",
      "|            215.23|\n",
      "|        212.000006|\n",
      "|        212.000006|\n",
      "|        213.000002|\n",
      "|209.76999500000002|\n",
      "|210.92999500000002|\n",
      "|210.45999700000002|\n",
      "|211.59999700000003|\n",
      "|215.18999900000003|\n",
      "|        215.549994|\n",
      "|213.30999599999998|\n",
      "|        207.499996|\n",
      "|        204.699999|\n",
      "|        213.710005|\n",
      "|            210.58|\n",
      "|        205.500004|\n",
      "|        202.199995|\n",
      "|             196.0|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('High').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|              High|             Close|\n",
      "+------------------+------------------+\n",
      "|        214.499996|        214.009998|\n",
      "|        215.589994|        214.379993|\n",
      "|            215.23|        210.969995|\n",
      "|        212.000006|            210.58|\n",
      "|        212.000006|211.98000499999998|\n",
      "|        213.000002|210.11000299999998|\n",
      "|209.76999500000002|        207.720001|\n",
      "|210.92999500000002|        210.650002|\n",
      "|210.45999700000002|            209.43|\n",
      "|211.59999700000003|            205.93|\n",
      "|215.18999900000003|        215.039995|\n",
      "|        215.549994|            211.73|\n",
      "|213.30999599999998|        208.069996|\n",
      "|        207.499996|            197.75|\n",
      "|        204.699999|        203.070002|\n",
      "|        213.710005|        205.940001|\n",
      "|            210.58|        207.880005|\n",
      "|        205.500004|        199.289995|\n",
      "|        202.199995|        192.060003|\n",
      "|             196.0|        194.729998|\n",
      "+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['High','Close']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### The `limit` method to take first few rows (without any collection)\n",
    "\n",
    "Applying `limit()` to the DataFrame will result in a new Dataframe. This is a transformation and does not perform collecting the data. Other similar methods like `take` and `head` result in an Array of Rows i.e. a Scala Collection Object like `scala.collection.immutable.Array` (which is transformed to Python list while using the PySpark API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### The `head()` and the `asDict()` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2010, 1, 4), Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039),\n",
       " Row(Date=datetime.date(2010, 1, 5), Open=214.599998, High=215.589994, Low=213.249994, Close=214.379993, Volume=150476200, Adj Close=27.774976000000002)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1=df.head(2)[0].asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': datetime.date(2010, 1, 4),\n",
       " 'Open': 213.429998,\n",
       " 'High': 214.499996,\n",
       " 'Low': 212.38000099999996,\n",
       " 'Close': 214.009998,\n",
       " 'Volume': 123432400,\n",
       " 'Adj Close': 27.727039}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### The `count` method - number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1762"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Coutning rows with distinct values - `distinct` method\n",
    "\n",
    "To demonstrate this method, we read in another small CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Company| Person|Sales|\n",
      "+-------+-------+-----+\n",
      "|   GOOG|    Sam|200.0|\n",
      "|   GOOG|Charlie|120.0|\n",
      "|   GOOG|  Frank|340.0|\n",
      "|   MSFT|   Tina|600.0|\n",
      "|   MSFT|    Amy|124.0|\n",
      "|   MSFT|Vanessa|243.0|\n",
      "|     FB|   Carl|870.0|\n",
      "|     FB|  Sarah|350.0|\n",
      "|   APPL|   John|250.0|\n",
      "|   APPL|  Linda|130.0|\n",
      "|   APPL|   Mike|750.0|\n",
      "|   APPL|  Chris|350.0|\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark1.read.csv('../datasets/sparkbyexamples/sales_info.csv',inferSchema=True,header=True)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|Company|\n",
      "+-------+\n",
      "|   APPL|\n",
      "|   GOOG|\n",
      "|     FB|\n",
      "|   MSFT|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select('Company').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.select('Company').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Random sampling using the `sample` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-02-03|        195.169994|        200.200003|        194.420004|        199.229994|153832000|25.812148999999998|\n",
      "|2010-05-05|         253.03001|258.14000699999997|248.72999199999998|255.98999799999999|220775800|         33.165949|\n",
      "|2010-09-20|         276.07999|        283.780006|275.84999799999997|        283.230007|164669400|36.695153000000005|\n",
      "|2010-10-19|303.40000200000003|        313.770012|300.02000400000003|309.48999399999997|308196000|40.097384999999996|\n",
      "|2010-10-21|312.35999300000003|314.73999399999997|        306.799999|        309.520008|137865000|         40.101273|\n",
      "|2011-03-22|342.55998999999997|        342.619991|        339.139992|        341.200012| 81480700|          44.20572|\n",
      "|2012-03-27|        606.180016|        616.280006|        606.060013| 614.4800190000001|151782400|         79.611755|\n",
      "|2012-08-08|        619.389984| 623.8800200000001|        617.099998|        619.860008| 61176500|         80.308784|\n",
      "|2012-12-11|        539.770004|        549.559975|        537.370003| 541.3900150000001|148086400| 70.76514300000001|\n",
      "|2014-02-24|        523.150024|        529.920006|        522.420021|        527.550018| 72227400| 71.11148299999999|\n",
      "|2014-06-27|             90.82|              92.0|         90.769997|         91.980003| 64029000|         87.274325|\n",
      "|2016-12-08|        110.860001|            112.43|        110.599998|        112.120003| 27068300|111.63599599999999|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sample(withReplacement=False,\n",
    "          fraction = 0.005,\n",
    "          seed = 101).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Inserting new column using `withColumn` method\n",
    "\n",
    "We can insert a new column into the DataFrame using the `withColumn()` method. It is mostly used when we are doing some transformartion of the data using one or more columns. Following is a simple example where we are calculating the range of the stock price (difference between the `High` and `Low` columns).\n",
    "\n",
    "Also note how we are **chaining the methods** - a `withColumn`, followed by a `limit`, followed by a `select` to show only the columns impacted (in a certain order), and finally followed by a `show` to see the final result of only first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+\n",
      "|      High|               Low|             Range|\n",
      "+----------+------------------+------------------+\n",
      "|214.499996|212.38000099999996|2.1199950000000456|\n",
      "|215.589994|        213.249994|2.3400000000000034|\n",
      "|    215.23|        210.750004|          4.479996|\n",
      "|212.000006|        209.050005|2.9500010000000145|\n",
      "|212.000006|209.06000500000002| 2.940000999999995|\n",
      "+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('Range',df['High']-df['Low']).limit(5).select(['High','Low','Range']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **not an _in-place_ operation** i.e. the original DataFrame remians unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Renaming a column using `withColumnRenamed` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.withColumn('Range',df['High']-df['Low']).limit(5).select(['High','Low','Range']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+\n",
      "|      High|               Low|             Range|\n",
      "+----------+------------------+------------------+\n",
      "|214.499996|212.38000099999996|2.1199950000000456|\n",
      "|215.589994|        213.249994|2.3400000000000034|\n",
      "|    215.23|        210.750004|          4.479996|\n",
      "|212.000006|        209.050005|2.9500010000000145|\n",
      "|212.000006|209.06000500000002| 2.940000999999995|\n",
      "+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+\n",
      "|      High|               Low|        Min-to-Max|\n",
      "+----------+------------------+------------------+\n",
      "|214.499996|212.38000099999996|2.1199950000000456|\n",
      "|215.589994|        213.249994|2.3400000000000034|\n",
      "|    215.23|        210.750004|          4.479996|\n",
      "|212.000006|        209.050005|2.9500010000000145|\n",
      "|212.000006|209.06000500000002| 2.940000999999995|\n",
      "+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Just pass the old and new column names\n",
    "df3 = df3.withColumnRenamed('Range','Min-to-Max')\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Filtering Operations - using either `filter()` or `where()` method\n",
    "\n",
    "Filtering can be done with **SQL-like syntax** or **Pythonic** way. We show both examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass on a SQL syntax to the `filter()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Close < 500\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chain methods to see only desired columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------+\n",
      "|      Date|      Open|             Close|\n",
      "+----------+----------+------------------+\n",
      "|2012-02-15|514.259995|        497.669975|\n",
      "|2013-09-05|500.250008|495.26997400000005|\n",
      "|2013-09-10|506.199997|494.63999900000005|\n",
      "|2014-01-30|502.539993|        499.779984|\n",
      "+----------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Close < 500 AND Open > 500\").select(['Date','Open','Close']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+\n",
      "|      Date|              Open|             Close|\n",
      "+----------+------------------+------------------+\n",
      "|2010-07-21|        265.089993|254.23999799999999|\n",
      "|2011-03-16|        342.000004|         330.01001|\n",
      "|2011-08-04|        389.410007|        377.369999|\n",
      "|2011-09-29|        401.919987|        390.570007|\n",
      "|2011-11-10|397.02999500000004|385.22000499999996|\n",
      "+----------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(\"Open < 500 AND (Open-Close)> 10\").select(['Date','Open','Close']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Now we use DataFrame syntax to achieve the same output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Close']<500).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we need to chain multiple conditions together, use `&` for AND and `|` for OR and clearly separate the conditions by putting them inside parantheses.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------+\n",
      "|      Date|      Open|             Close|\n",
      "+----------+----------+------------------+\n",
      "|2012-02-15|514.259995|        497.669975|\n",
      "|2013-09-05|500.250008|495.26997400000005|\n",
      "|2013-09-10|506.199997|494.63999900000005|\n",
      "|2014-01-30|502.539993|        499.779984|\n",
      "+----------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Close']<500) & (df['Open']>500)).select(['Date','Open','Close']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+\n",
      "|      Date|              Open|             Close|\n",
      "+----------+------------------+------------------+\n",
      "|2010-07-21|        265.089993|254.23999799999999|\n",
      "|2011-03-16|        342.000004|         330.01001|\n",
      "|2011-08-04|        389.410007|        377.369999|\n",
      "|2011-09-29|        401.919987|        390.570007|\n",
      "|2011-11-10|397.02999500000004|385.22000499999996|\n",
      "+----------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df['Close']<500) & (df['Open']-df['Close']>10)).select(['Date','Open','Close']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `==` to compare with an exact value for comparison and `~` for NOT operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----------+------+------+---------+---------+\n",
      "|      Date|              Open|      High|   Low| Close|   Volume|Adj Close|\n",
      "+----------+------------------+----------+------+------+---------+---------+\n",
      "|2010-01-22|206.78000600000001|207.499996|197.16|197.75|220441900|25.620401|\n",
      "+----------+------------------+----------+------+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Low']==197.16).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter((df['Open']<200) & ~(df['Close']>200)).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Use the `collect` method instead of `show`, to collect the actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_data = df.filter(df['Low']==197.16).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2010, 1, 22), Open=206.78000600000001, High=207.499996, Low=197.16, Close=197.75, Volume=220441900, Adj Close=25.620401)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is still a list. So, grab the 0-index element as a Row object and convert it to a dictionary using `asDict` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = low_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': datetime.date(2010, 1, 22),\n",
       " 'Open': 206.78000600000001,\n",
       " 'High': 207.499996,\n",
       " 'Low': 197.16,\n",
       " 'Close': 197.75,\n",
       " 'Volume': 220441900,\n",
       " 'Adj Close': 25.620401}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can do whatever processing you want to do with the dictionary object!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Sorting DataFrame - `sort` and `orderBy` methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a smaller DataFrame from the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df.select('High','Low').limit(10)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.sort(\"High\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, it sorts by ascending order. You can change it by passing `ascending=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.sort(\"High\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the **`desc` function** imported from the `pyspark.sql.functions` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "df4.sort(desc(\"High\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass a list to the `ascending` parameter to create custom sorting order on various columns. In the following example, we are sorting in descending order for the `Company` column, while doing an ascending sort on the `Sales` column. We are using the `orderBy` method for this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.orderBy([\"Company\",\"Sales\"],ascending=[0,1]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Using functions from the `pyspark.sql.functions` module\n",
    "\n",
    "We can utilize various utility functions from the `pyspark.sql.functions` module to do transformation on the data and extract insights, look for patterns, build statistical models, etc.\n",
    "\n",
    "We will just show a couple of examples here, but you are encouraged to explore others. Here is the detailed documentation link: \n",
    "\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr, dayofweek, month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, we want to check ***if there is any correlation between the trade volumes and the range (Max - Min) i.e. intra-day volatility of the stock***.\n",
    "\n",
    "Here is how we can do that computation. Note the use of two new methods - `agg` which is nothing but an aggregration over the entire DataFrame without forming any specific groups and the `alias` method which is just a way to specify a descriptive name for the column resulting from the application of the `corr` function. We need the `agg` here because we want to aggregrate and throw all the data of the DataFrame at the `corr` function to calculate the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) between the `Volume` and `Range` data columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn('Range',df['High']-df['Low']).agg(corr(\"Volume\",\"Range\").alias('Pearson-Corr')).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we do have a fairly strong positive correlation between the `Volume` and `Range` data.\n",
    "\n",
    "Here is another fun observation! If you stare at the code above a bit longer, you will notice that **we did not produce a standalone `Range` column at all**. We computed it in an ephemeral manner - just enough to calculate the correlation coefficient. The original DataFrame remains unchanged while we got our answer about the positive correlation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply the `dayofweek` and `month` functions on the `Date` column to produce new columns `Day-of-Week` and `Month`. Note how we chained two `withColumn` methods together for this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=df.withColumn('Day-of-Week',dayofweek('Date')).\\\n",
    "withColumn('Month',month('Date')).\\\n",
    "select(['Day-of-Week','Month','Open','Close'])\n",
    "\n",
    "df5.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have not shown the `groupby()` and aggregration (like average or `mean`) methods yet but if you have a fair understanding of what a `groupby` (followed by an aggregration like `mean`) does, then you can appreciate the following example where we look at the average `Volume` grouped by `Day-of-Week` and `Month` and try to gauge ***which day of the week or month have the highest and lowest average trade volumes***.\n",
    "\n",
    "The example below is certainly a complex one but you can think this one as an illustration of using some of the most important DataFrame methods we have examined in this Notebook (and some future ones we will look at), in a combined fashion.\n",
    "\n",
    "- `withColumn`\n",
    "- `select`\n",
    "- `orderBy`\n",
    "- `groupBy`\n",
    "- `mean` (aggregration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=df.withColumn('Day-of-Week',dayofweek('Date')).\\\n",
    "withColumn('Month',month('Date')).\\\n",
    "select(['Day-of-Week','Month','Volume']).\\\n",
    "groupby(['Day-of-Week','Month']).\\\n",
    "mean().\\\n",
    "select(['Day-of-Week','Month','avg(Volume)']).\\\n",
    "orderBy(['Day-of-Week','Month'])\n",
    "\n",
    "df5.show(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
